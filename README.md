# 🧠 Deep Neural Network from Scratch (NumPy Implementation)

Welcome to my hands-on implementation of a **Deep Neural Network (DNN)** built entirely **from scratch using NumPy**. This project demonstrates a solid understanding of the inner workings of neural networks — without relying on any high-level libraries like TensorFlow or PyTorch.

> 📌 This repository is a key part of my deep learning journey, and reflects my commitment to mastering AI fundamentals through first-principles implementation.

---

## 🚀 What’s Inside?

This project implements a **multi-layer fully-connected neural network** with the following features:

- Layer-wise forward and backward propagation
- Vectorized computations using NumPy
- ReLU and Sigmoid activation functions
- Cross-entropy loss calculation
- Gradient descent optimization
- Modular and extensible code structure

---

## 📂 Directory Structure

```text
deep-nn-from-scratch/
│
├── deep_nn.ipynb       # Main notebook: builds & trains the deep neural network
├── dnn_utils.py        # Utility module: activations, forward/backward passes, plotting
│
├── datasets/           # Contains training/test .h5 files (cat vs. non-cat)
├── images/             # Diagrams or visualizations used in the notebook

```

---

## 📈 Training Summary

- **Task:** Binary classification (e.g., Cat vs Non-Cat)
- **Network Depth:** Configurable
- **Optimized with:** Gradient Descent

---

## DNN Architecture

<img src="images/my_nn.png" alt="DNN Architecture" style="width:700px; height:400px; display:block; margin:auto;"/>

---

## 💡 Key Concepts Practiced

| Topic                        | Skills Demonstrated                       |
|-----------------------------|-------------------------------------------|
| Neural Networks             | Architecture, activations, forward pass  |
| Backpropagation             | Manual gradient calculations              |
| Visuals                     |  Plot learning curves (loss vs epochs)    |
| NumPy                       | Efficient vectorized math                 |
| Debugging & Modularity      | Clean, testable, reusable design          |

---

## 🛠️ Tech Stack

- **Language:** Python 3
- **Libraries:** NumPy, Matplotlib (for visualization)

---

## 🤖 Future Work

- ⏳ Add mini-batch gradient descent
- ⏳ Extend to multiclass classification
- ⏳ Visualize hidden layer activations
- ⏳  Add Regularization Techniques

---

## 📌 Why This Matters

This project isn’t just about coding - it’s about **building true understanding** of how deep learning works under the hood. It reflects:
- My commitment to learning AI from first principles
- My ability to write clean, scalable, vectorized code

---

## 👨‍💻 Author

**Nabeel Shan**  
📚 Software Engineering Student - NUST Islamabad  
🔬 Aspiring AI Researcher | AI/ML Enthusiast  
🌐 [LinkedIn](https://www.linkedin.com/in/nabeelshan) • [GitHub](https://github.com/nabeelshan78)

---

## ⭐️ Star This Repo

If you found this helpful or interesting, feel free to **star this repo** and follow my journey as I implement more ML and DL algorithms from scratch.

---

## 📬 Feedback

I’m always looking to improve! Feel free to open an issue or send me feedback on how I can improve this repo or its documentation.

