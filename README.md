# ğŸ§  Deep Neural Network from Scratch  

> Built entirely using NumPy â€” No frameworks. 100% custom implementation.

---

## Introduction

<img src="images/my_nn.png" width="600"/>

This repository showcases a fully functional deep neural network **built entirely from scratch** using **NumPy**. Every component - forward/backward propagation, weight updates, activations, regularization, and optimization - is manually implemented. No high-level ML/DL libraries (like TensorFlow or PyTorch) are used.  

> This project serves as a demonstration of my deep theoretical knowledge of deep learning and strong NumPy/Python coding abilities.

---

## âœ… Features

- **Architecture:**
  - Modular, multi-layer feedforward neural network
  - Forward and Backward propagation from scratch  
  - Dense (fully connected) layers

- **Core Modules:**
  - Activations: `Sigmoid`, `ReLU`
  - Losses: `Binary Cross-Entropy`
  - Optimizers: `GD`, `Momentum`, `RMSprop`, `Adam`
  - Initializations: `Random`, `Xavier (Glorot)`, `He`, `zero`

- **Training Dynamics:**
  - Mini-batch gradient descent
  - Dropout regularization
  - L2 weight regularization

- **Visualization & Debugging Tools:**
  - Training loss curves, decision boundaries
  - Optimizer/initialization/regularization comparisons
  - Detailed architecture diagrams

---

## Why From Scratch?

> *"If you really want to understand deep learning, implement it yourself."*

Most deep learning engineers rely on high-level libraries. But building it all from scratch:
- Forces you to master the **underlying math** (linear algebra, calculus)
- Makes backpropagation **intuitive**, not magical
- Prepares you to debug and optimize real models
- Proves you understand every part of how deep networks work

This project represents that journey â€” from pure math to working code.

---

## ğŸ§© Project Structure

<pre>

deep-nn-from-scratch/
â”‚
â”œâ”€â”€ datasets/                   # Training/testing datasets
â”‚   â”œâ”€â”€ train_catvnoncat.h5
â”‚   â””â”€â”€ test_catvnoncat.h5
â”‚
â”œâ”€â”€ images/                     # Visualizations and architecture diagrams
â”‚   â”œâ”€â”€ backprop.png
â”‚   â”œâ”€â”€ batch_minibatches_comp.png
â”‚   â”œâ”€â”€ cost-vs-epoch-cat-data.png
â”‚   â”œâ”€â”€ dropout_comp.png
â”‚   â”œâ”€â”€ inits_comparision.png
â”‚   â”œâ”€â”€ l2_comp.png
â”‚   â”œâ”€â”€ learn_rates_comp.png
â”‚   â”œâ”€â”€ linearback.png
â”‚   â”œâ”€â”€ mood_data_dec_boundary.png
â”‚   â”œâ”€â”€ moon_data_cost_epochs.png
â”‚   â”œâ”€â”€ my_nn.png
â”‚   â”œâ”€â”€ optimizers_comp.png
â”‚   â””â”€â”€ structure.png
â”‚
â”œâ”€â”€ model/                      # Core deep learning components
â”‚   â”œâ”€â”€ forward_propagation.py
â”‚   â”œâ”€â”€ backward_propagation.py
â”‚   â”œâ”€â”€ initialization.py
â”‚   â”œâ”€â”€ update.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ predict.py
â”‚
â”œâ”€â”€ demo_notebook.ipynb         # End-to-end training and visualization demo
â”œâ”€â”€ DNN_Math_And_Theory.ipynb   # Theory + derivations for NN math
â”œâ”€â”€ train.py                    # Script to run training pipeline
â””â”€â”€ README.md                   # This file

</pre>




---

## ğŸ“˜ Theoretical Notebook: `DNN_Math_And_Theory.ipynb`

This notebook derives the math behind neural networks from scratch:
- Matrix calculus for gradients
- Backpropagation derivation
- Layer-wise forward/backward pass
- Activation derivatives
- Cost function intuition

> ğŸ§  Use this to understand **why** each line of code exists, not just what it does.

---

## ğŸ“¸ Visual Results & Comparisons

This section showcases the **performance, behavior, and internal workings** of the neural network across various techniques and experiments.

---

### Architecture & Gradient Flow
- **Model Structure Overview**
- Visual explanation of forward/backward propagation paths.

<img src="images/structure.png" width="600" height="400"/>
<img src="images/backprop.png" width="500"/>

---

### Training Dynamics
                                                                                                                                                                                                  
#### ğŸ“ˆ Cost vs Epochs
- Loss curve during training on the cat vs. non-cat dataset.

<img src="images/cost-vs-epoch-cat-data.png" width="500"/>

#### ğŸš€ Learning Rate Impact
- Comparing performance across different learning rates.

<img src="images/learn_rates_comp.png" width="500"/>

---

### ğŸ”„ Optimizer Comparison
- Effectiveness of different optimizers: **GD**, **Momentum**, **RMSProp**, **Adam**.

<img src="images/optimizers_comp.png" width="500"/>

---

### ğŸ¯ Weight Initialization Methods
- Comparison between **Random**, **He**, and **Xavier/Glorot** initializations.

<img src="images/inits_comparision.png" width="500"/>

---

### ğŸ›¡ï¸ Regularization Techniques

#### Dropout Regularization
- Prevents overfitting by randomly disabling neurons during training.

<img src="images/dropout_comp.png" width="450"/>

#### L2 Regularization
- Penalizes large weights to improve generalization.

<img src="images/l2_comp.png" width="450"/>

---

### ğŸ§© Batch Strategies

#### Batch vs Mini-Batch
- Efficiency and convergence comparison between training with full batch and mini-batches.

<img src="images/batch_minibatches_comp.png" width="500"/>

---

### ğŸŒ™ Decision Boundaries

- Real-time visualizations of learned decision boundaries on synthetic data (e.g., moons).

<img src="images/mood_data_dec_boundary.png" width="500"/>

---



Each image reflects a real experiment run with this framework. These visuals make the networkâ€™s internal learning process **transparent** and interpretable.

---

## ğŸ’» Getting Started

### 1. Clone this repository:
```bash
git clone https://github.com/nabeelshan78/deep-nn-from-scratch.git
cd deep-nn-from-scratch
```

## ğŸ‘¨â€ğŸ’» Author

**Nabeel Shan**  
ğŸ“š Software Engineering Student - NUST Islamabad  
ğŸ”¬ Aspiring AI Researcher | AI/ML Enthusiast  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/nabeelshan) â€¢ [GitHub](https://github.com/nabeelshan78)

---

## â­ï¸ Star This Repo

If you found this helpful or interesting, feel free to **star this repo** and follow my journey as I implement more ML and DL algorithms from scratch.

---

## ğŸ“¬ Feedback

Iâ€™m always looking to improve! Feel free to open an issue or send me feedback on how I can improve this repo or its documentation.

