# ğŸ§  Deep Neural Network from Scratch (NumPy Implementation)

Welcome to my hands-on implementation of a **Deep Neural Network (DNN)** built entirely **from scratch using NumPy**. This project demonstrates a solid understanding of the inner workings of neural networks â€” without relying on any high-level libraries like TensorFlow or PyTorch.

> ğŸ“Œ This repository is a key part of my deep learning journey, and reflects my commitment to mastering AI fundamentals through first-principles implementation.

---

## ğŸš€ Whatâ€™s Inside?

This project implements a **multi-layer fully-connected neural network** with the following features:

- Layer-wise forward and backward propagation
- Vectorized computations using NumPy
- ReLU and Sigmoid activation functions
- Cross-entropy loss calculation
- Gradient descent optimization
- Modular and extensible code structure

---

## ğŸ“‚ Directory Structure

```text
deep-nn-from-scratch/
â”‚
â”œâ”€â”€ deep_nn.ipynb       # Main notebook: builds & trains the deep neural network
â”œâ”€â”€ dnn_utils.py        # Utility module: activations, forward/backward passes, plotting
â”‚
â”œâ”€â”€ datasets/           # Contains training/test .h5 files (cat vs. non-cat)
â”œâ”€â”€ images/             # Diagrams or visualizations used in the notebook

```

---

## ğŸ“ˆ Training Summary

- **Task:** Binary classification (e.g., Cat vs Non-Cat)
- **Network Depth:** Configurable
- **Optimized with:** Gradient Descent

---

## DNN Architecture

<img src="images/my_nn.png" alt="DNN Architecture" style="width:700px; height:400px; display:block; margin:auto;"/>

---

## ğŸ’¡ Key Concepts Practiced

| Topic                        | Skills Demonstrated                       |
|-----------------------------|-------------------------------------------|
| Neural Networks             | Architecture, activations, forward pass  |
| Backpropagation             | Manual gradient calculations              |
| Visuals                     |  Plot learning curves (loss vs epochs)    |
| NumPy                       | Efficient vectorized math                 |
| Debugging & Modularity      | Clean, testable, reusable design          |

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python 3
- **Libraries:** NumPy, Matplotlib (for visualization)

---

## ğŸ¤– Future Work

- â³ Add mini-batch gradient descent
- â³ Extend to multiclass classification
- â³ Visualize hidden layer activations
- â³  Add Regularization Techniques

---

## ğŸ“Œ Why This Matters

This project isnâ€™t just about coding - itâ€™s about **building true understanding** of how deep learning works under the hood. It reflects:
- My commitment to learning AI from first principles
- My ability to write clean, scalable, vectorized code

---

## ğŸ‘¨â€ğŸ’» Author

**Nabeel Shan**  
ğŸ“š Software Engineering Student - NUST Islamabad  
ğŸ”¬ Aspiring AI Researcher | AI/ML Enthusiast  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/nabeelshan) â€¢ [GitHub](https://github.com/nabeelshan78)

---

## â­ï¸ Star This Repo

If you found this helpful or interesting, feel free to **star this repo** and follow my journey as I implement more ML and DL algorithms from scratch.

---

## ğŸ“¬ Feedback

Iâ€™m always looking to improve! Feel free to open an issue or send me feedback on how I can improve this repo or its documentation.

